{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "col = MongoClient(\"127.0.0.1\", 27017)[\"dlsc\"][\"distribution_metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(\n",
    "    r\"(github\\.com|bitbucket\\.org|gitlab\\.com)/[a-zA-Z0-9_\\.\\-]+/[a-zA-Z0-9_\\.\\-]+\"\n",
    ")\n",
    "\n",
    "def parse_metadata(metadata):\n",
    "    if not metadata:\n",
    "        return None\n",
    "    home_page = metadata.get(\"home_page\")\n",
    "    download_url = metadata.get(\"download_url\")\n",
    "    project_urls = metadata.get(\"project_urls\", [])\n",
    "    if home_page:\n",
    "        match = pattern.search(home_page)\n",
    "        if match:\n",
    "            return \"https://\" + match.group(0)\n",
    "    if download_url:\n",
    "        match = pattern.search(download_url)\n",
    "        if match:\n",
    "            return \"https://\" + match.group(0)\n",
    "    if project_urls:\n",
    "        for url in project_urls:\n",
    "            match = pattern.search(url)\n",
    "            if match:\n",
    "                return \"https://\" + match.group(0)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5743721it [00:41, 138906.53it/s]\n"
     ]
    }
   ],
   "source": [
    "pkg_repo_urls = {}\n",
    "cnt = 0\n",
    "\n",
    "for metadata in tqdm(col.find({}, projection={\"_id\": 0, \"name\": 1, \"home_page\": 1, \"download_url\": 1, \"project_urls\": 1})):\n",
    "    # cnt += 1\n",
    "    # if cnt == 66161:\n",
    "    #     # print(metadata)\n",
    "    #     # break\n",
    "    name = metadata[\"name\"].lower()\n",
    "    if not pkg_repo_urls.get(name):\n",
    "        pkg_repo_urls[name] = parse_metadata(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354636, 243534)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pkg_repo_urls), len([v for v in pkg_repo_urls.values() if v is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3132846073156702"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - len([v for v in pkg_repo_urls.values() if v is not None]) / len(pkg_repo_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging.version import Version\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "import wget\n",
    "import requests\n",
    "\n",
    "db = MongoClient(\"127.0.0.1\", port=27017)[\"dlsc\"]\n",
    "\n",
    "\n",
    "def download_dists(sampled_dependents, sc):\n",
    "    node_col = db[f\"{sc}_nodes\"]\n",
    "    edge_col = db[f\"{sc}_edges\"]\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for samp in sampled_dependents:\n",
    "        print(f\"Start {samp}\")\n",
    "        versions = [v[\"version\"] for v in node_col.find({\"name\": samp})]\n",
    "        latest_version = sorted(versions, key=lambda x: Version(x))[-1]\n",
    "\n",
    "        deps = edge_col.find({\"name\": samp, \"version\": latest_version}).distinct(\"dependency\")\n",
    "\n",
    "        response = requests.get(f\"https://pypi.org/pypi/{samp}/{latest_version}/json\")\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                urls = response.json()[\"urls\"]\n",
    "                for data in urls:\n",
    "                    filename = data['filename']\n",
    "                    url = data['url']\n",
    "                    packagetype = data['packagetype']\n",
    "                    if packagetype in [\"bdist_wheel\", \"bdist_egg\"]:\n",
    "                        break\n",
    "                wget.download(url, f\"dists/{sc}/{filename}\")\n",
    "                for d in deps:\n",
    "                    res.append([samp, latest_version, d, filename])\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {samp} {latest_version}\")\n",
    "                print(e)\n",
    "                for d in deps:\n",
    "                    res.append([samp, latest_version, d, None])\n",
    "        else:\n",
    "            print(f\"404: {samp} {latest_version}\")\n",
    "            for d in deps:\n",
    "                res.append([samp, latest_version, d, None])\n",
    "        print(f\"Finish {samp}\")\n",
    "    return res\n",
    "\n",
    "def sample_packages(sc: str, k: int):\n",
    "    df = pd.read_csv(f\"data/{sc}/{sc}_edges.csv\", names=[\"dependent\", \"dependency\"])\n",
    "    dependents = list(df[\"dependent\"].unique())\n",
    "    sampled_dependents = sample(dependents, k)\n",
    "\n",
    "    res = download_dists(sampled_dependents, sc)\n",
    "    pd.DataFrame(res, columns=[\"name\", \"version\", \"dependencies\", \"filepath\"]).to_csv(f\"data/{sc}/{sc}_sampled.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_packages(\"TensorFlow\", 334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_packages(\"PyTorch\", 344)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import requests\n",
    "import wget\n",
    "def get_import_name(name: str):\n",
    "    url = f\"https://pypi.org/pypi/{name}/json\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()[\"urls\"]\n",
    "            for d in data:\n",
    "                if d[\"packagetype\"] == \"bdist_wheel\":\n",
    "                    path = f\"dists/{d['filename']}\"\n",
    "                    wget.download(d[\"url\"], path)\n",
    "                    zipf = zipfile.ZipFile(path, 'r')\n",
    "                    files = zipf.namelist()\n",
    "                    for f in files:\n",
    "                        if f.endswith(\"top_level.txt\"):\n",
    "                            with zipf.open(f) as f:\n",
    "                                return f.read().decode(\"utf-8\").split(\"\\n\")[:-1]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df = pd.read_csv(\"data/TensorFlow/TensorFlow_sampled.csv\")\n",
    "tf_import_names = {'tensorflow': ['tensorflow'],\n",
    " 'tensorflow-gpu': ['tensorflow'],\n",
    " 'tensorflow-probability': ['tensorflow_probability'],\n",
    " 'tensorflow-addons': ['tensorflow_addons'],\n",
    " 'tensorflow-text': ['tensorflow_text'],\n",
    " 'streamlit': ['steamlit'],\n",
    " 'tensorflow-cpu': ['tensorflow'],\n",
    "}\n",
    "\n",
    "for name in tf_df[\"dependencies\"].value_counts().index:\n",
    "    if name not in tf_import_names:\n",
    "        tf_import_names[name] = get_import_name(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_import_names['dnnv'] = 'dnnv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_df = pd.read_csv(\"data/PyTorch/PyTorch_sampled.csv\")\n",
    "pt_import_names = {'torch': ['torch'],\n",
    " 'torchvision': ['torchvision'],\n",
    " 'pytorch-lightning': ['pytorch_lightning'],\n",
    " 'torchtext': ['torchtext'],\n",
    " 'fastai': ['fastai'],\n",
    " 'torchaudio': ['torchaudio'],\n",
    "}\n",
    "\n",
    "for name in pt_df[\"dependencies\"].value_counts().index:\n",
    "    if name not in pt_import_names:\n",
    "        pt_import_names[name] = get_import_name(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import tarfile\n",
    "import ast\n",
    "\n",
    "def read_dist_file(filepath: str):\n",
    "    if filepath.endswith(\".whl\") or filepath.endswith(\".egg\"):\n",
    "        zipf = zipfile.ZipFile(filepath)\n",
    "        files = {name: zipf.open(name).read().decode(\"utf-8\") for name in zipf.namelist() if name.endswith(\".py\")}\n",
    "        return files\n",
    "    elif filepath.endswith(\".tar.gz\"):\n",
    "        tarf = tarfile.open(filepath)\n",
    "        files = {name: tarf.extractfile(name).read().decode(\"utf-8\") for name in tarf.getnames() if name.endswith(\".py\")}\n",
    "        return files\n",
    "    return {}\n",
    "    \n",
    "def parse_imports(import_names: list, file: str):\n",
    "    f_ast = ast.parse(file)\n",
    "    res = []\n",
    "    for node in ast.walk(f_ast):\n",
    "        if isinstance(node, ast.ImportFrom):\n",
    "            module = node.module\n",
    "            if not node.module:\n",
    "                continue\n",
    "            module = module.split(\".\")[0]\n",
    "            if module in import_names:\n",
    "                res.append(node.lineno)\n",
    "        elif isinstance(node, ast.Import):\n",
    "            module_names = [alias.name for alias in node.names]\n",
    "            if set(module_names).intersection(set(import_names)):\n",
    "                res.append(node.lineno)\n",
    "    return tuple(res)\n",
    "\n",
    "def parse_dist(x, sc: str):\n",
    "    if sc == \"TensorFlow\":\n",
    "        import_names = tf_import_names\n",
    "    elif sc == \"PyTorch\":\n",
    "        import_names = pt_import_names\n",
    "\n",
    "    dependencies = x[\"dependencies\"]\n",
    "    res = []\n",
    "    if x[\"filepath\"]:\n",
    "        filepath = f\"dists/{sc}/{x['filepath']}\"\n",
    "        files = read_dist_file(filepath)\n",
    "        for file_name, file_content in files.items():\n",
    "            try:\n",
    "                linenos = parse_imports(import_names[dependencies], file_content)\n",
    "                if linenos:\n",
    "                    res.append((file_name, linenos))\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {filepath} {file_name}\")\n",
    "                print(e)\n",
    "\n",
    "    return res\n",
    "\n",
    "def parse_dists(sc: str):\n",
    "    df = pd.read_csv(f\"data/{sc}/{sc}_sampled.csv\")\n",
    "    for _, row in df.iterrows():\n",
    "        res = parse_dist(row, sc)\n",
    "        df.iloc[_, 4] = res\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df = parse_dists(\"TensorFlow\")\n",
    "tf_df.to_csv(f\"data/TensorFlow/TensorFlow_sampled.csv\", index=False)\n",
    "pt_df = parse_dists(\"PyTorch\")\n",
    "pt_df.to_csv(f\"data/PyTorch/PyTorch_sampled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 327 305\n",
      "344 338 334\n"
     ]
    }
   ],
   "source": [
    "tf_df = pd.read_csv(\"data/TensorFlow/TensorFlow_sampled.csv\")\n",
    "pt_df = pd.read_csv(\"data/PyTorch/PyTorch_sampled.csv\")\n",
    "\n",
    "print(len(tf_df[\"name\"].unique()), \n",
    "      len(tf_df[tf_df[\"filepath\"].notna()][\"name\"].unique()), \n",
    "      len(tf_df[tf_df[\"filepath\"].notna() & (tf_df[\"imports\"].str.len() > 2)][\"name\"].unique())\n",
    ")\n",
    "\n",
    "print(len(pt_df[\"name\"].unique()), \n",
    "      len(pt_df[pt_df[\"filepath\"].notna()][\"name\"].unique()), \n",
    "      len(pt_df[pt_df[\"filepath\"].notna() & (pt_df[\"imports\"].str.len() > 2)][\"name\"].unique())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
